{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f52ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7723/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77243f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 13:26:22.978481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-18 13:26:22.978519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-18 13:26:22.979450: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-18 13:26:22.986575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 13:26:23.771236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fashion_mnist=keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc05906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(A):\n",
    "    return A * (1 - A)\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def tanh_derivative(Z):\n",
    "    return 1 - np.tanh(Z)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75cdd08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, activation='relu'):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_name = activation\n",
    "        self.activation, self.activation_derivative = self.set_activation_functions(activation)\n",
    "        self.parameters = self.initialize_parameters()\n",
    "\n",
    "    def set_activation_functions(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return relu, relu_derivative\n",
    "        elif activation == 'sigmoid':\n",
    "            return sigmoid, sigmoid_derivative\n",
    "        elif activation == 'tanh':\n",
    "            return tanh, tanh_derivative\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "            \n",
    "    def initialize_parameters(self):\n",
    "        parameters = {}\n",
    "        for l in range(1, len(self.layer_sizes)):\n",
    "            parameters['W' + str(l)] = np.random.randn(self.layer_sizes[l], self.layer_sizes[l-1]) * 0.01\n",
    "            parameters['b' + str(l)] = np.zeros((self.layer_sizes[l], 1))\n",
    "        return parameters\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "    \n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        m = Y.shape[1]\n",
    "        loss = -np.sum(Y * np.log(Y_hat + 1e-9)) / m\n",
    "        return loss\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        caches = {}\n",
    "        A = X\n",
    "        L = len(self.parameters) // 2\n",
    "        \n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            Z = np.dot(self.parameters['W' + str(l)], A_prev) + self.parameters['b' + str(l)]\n",
    "            A = self.activation(Z) \n",
    "            caches['Z' + str(l)] = Z\n",
    "            caches['A' + str(l)] = A\n",
    "        \n",
    "        ZL = np.dot(self.parameters['W' + str(L)], A) + self.parameters['b' + str(L)]\n",
    "        AL = self.softmax(ZL)\n",
    "        caches['Z' + str(L)] = ZL\n",
    "        caches['A' + str(L)] = AL\n",
    "        return AL, caches\n",
    "    \n",
    "    def backpropagation(self, X, Y, caches):\n",
    "        grads = {}\n",
    "        L = len(self.parameters) // 2 # Number of layers\n",
    "        m = X.shape[1]\n",
    "        Y = Y.reshape(caches['A' + str(L)].shape) # Ensure same shape as output layer\n",
    "\n",
    "        # Initializing backpropagation and Output layer gradient\n",
    "        dZL = caches['A' + str(L)] - Y\n",
    "        grads[\"dW\" + str(L)] = 1./m * np.dot(dZL, caches['A' + str(L-1)].T)\n",
    "        grads[\"db\" + str(L)] = 1./m * np.sum(dZL, axis=1, keepdims=True)\n",
    "\n",
    "        for l in reversed(range(1, L)):\n",
    "            dA = np.dot(self.parameters[\"W\" + str(l+1)].T, dZL) # dA_prev\n",
    "            dZ = self.activation_derivative(caches['Z' + str(l)]) * dA # Element wise multiplication between 2 vectors\n",
    "            if l > 1:\n",
    "                grads[\"dW\" + str(l)] = 1./m * np.dot(dZ, caches['A' + str(l-1)].T)\n",
    "            else: # For the first hidden layer, use X as A0\n",
    "                grads[\"dW\" + str(l)] = 1./m * np.dot(dZ, X.T)\n",
    "            grads[\"db\" + str(l)] = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "            dZL = dZ  # For the next iteration. Prepare dZL for next layer (if not the first layer)\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        L = len(self.parameters) // 2\n",
    "        for l in range(L):\n",
    "            self.parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "            self.parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7201b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_one_hot(labels, classes):\n",
    "    return np.eye(classes)[labels].T\n",
    "\n",
    "def preprocess_data(train_images, train_labels, test_images, test_labels):\n",
    "    X_train = train_images.reshape(train_images.shape[0], -1).T / 255.\n",
    "    X_test = test_images.reshape(test_images.shape[0], -1).T / 255.\n",
    "    \n",
    "    Y_train = convert_labels_to_one_hot(train_labels, 10)\n",
    "    Y_test = convert_labels_to_one_hot(test_labels, 10)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def plot_training_loss_and_test_acc(epochs, traing_loss, test_accuracy):\n",
    "    epochs_range = list(range(0, epochs))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, traing_loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, test_accuracy, label='Test Accuracy')\n",
    "    plt.title('Training Loss and Test Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4749d5d9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss: 2.303152\n",
      "Test accuracy : 0.1867\n",
      "\n",
      "Epoch 1, Training loss: 2.299979\n",
      "Test accuracy : 0.2292\n",
      "\n",
      "Epoch 2, Training loss: 2.296831\n",
      "Test accuracy : 0.2615\n",
      "\n",
      "Epoch 3, Training loss: 2.293613\n",
      "Test accuracy : 0.2723\n",
      "\n",
      "Epoch 4, Training loss: 2.290235\n",
      "Test accuracy : 0.2728\n",
      "\n",
      "Epoch 5, Training loss: 2.286611\n",
      "Test accuracy : 0.2728\n",
      "\n",
      "Epoch 6, Training loss: 2.282653\n",
      "Test accuracy : 0.2684\n",
      "\n",
      "Epoch 7, Training loss: 2.278273\n",
      "Test accuracy : 0.2653\n",
      "\n",
      "Epoch 8, Training loss: 2.273381\n",
      "Test accuracy : 0.2661\n",
      "\n",
      "Epoch 9, Training loss: 2.267882\n",
      "Test accuracy : 0.2715\n",
      "\n",
      "Epoch 10, Training loss: 2.261677\n",
      "Test accuracy : 0.2764\n",
      "\n",
      "Epoch 11, Training loss: 2.254663\n",
      "Test accuracy : 0.2837\n",
      "\n",
      "Epoch 12, Training loss: 2.246732\n",
      "Test accuracy : 0.2919\n",
      "\n",
      "Epoch 13, Training loss: 2.237769\n",
      "Test accuracy : 0.3027\n",
      "\n",
      "Epoch 14, Training loss: 2.227656\n",
      "Test accuracy : 0.3159\n",
      "\n",
      "Epoch 15, Training loss: 2.216270\n",
      "Test accuracy : 0.3305\n",
      "\n",
      "Epoch 16, Training loss: 2.203492\n",
      "Test accuracy : 0.3491\n",
      "\n",
      "Epoch 17, Training loss: 2.189203\n",
      "Test accuracy : 0.3722\n",
      "\n",
      "Epoch 18, Training loss: 2.173301\n",
      "Test accuracy : 0.3888\n",
      "\n",
      "Epoch 19, Training loss: 2.155705\n",
      "Test accuracy : 0.4025\n",
      "\n",
      "Epoch 20, Training loss: 2.136368\n",
      "Test accuracy : 0.4143\n",
      "\n",
      "Epoch 21, Training loss: 2.115287\n",
      "Test accuracy : 0.4271\n",
      "\n",
      "Epoch 22, Training loss: 2.092516\n",
      "Test accuracy : 0.444\n",
      "\n",
      "Epoch 23, Training loss: 2.068163\n",
      "Test accuracy : 0.4612\n",
      "\n",
      "Epoch 24, Training loss: 2.042398\n",
      "Test accuracy : 0.4825\n",
      "\n",
      "Epoch 25, Training loss: 2.015441\n",
      "Test accuracy : 0.4966\n",
      "\n",
      "Epoch 26, Training loss: 1.987550\n",
      "Test accuracy : 0.5084\n",
      "\n",
      "Epoch 27, Training loss: 1.959010\n",
      "Test accuracy : 0.5188\n",
      "\n",
      "Epoch 28, Training loss: 1.930109\n",
      "Test accuracy : 0.5234\n",
      "\n",
      "Epoch 29, Training loss: 1.901121\n",
      "Test accuracy : 0.5227\n",
      "\n",
      "Epoch 30, Training loss: 1.872294\n",
      "Test accuracy : 0.519\n",
      "\n",
      "Epoch 31, Training loss: 1.843839\n",
      "Test accuracy : 0.514\n",
      "\n",
      "Epoch 32, Training loss: 1.815921\n",
      "Test accuracy : 0.5106\n",
      "\n",
      "Epoch 33, Training loss: 1.788664\n",
      "Test accuracy : 0.5083\n",
      "\n",
      "Epoch 34, Training loss: 1.762151\n",
      "Test accuracy : 0.5056\n",
      "\n",
      "Epoch 35, Training loss: 1.736431\n",
      "Test accuracy : 0.5043\n",
      "\n",
      "Epoch 36, Training loss: 1.711526\n",
      "Test accuracy : 0.5037\n",
      "\n",
      "Epoch 37, Training loss: 1.687438\n",
      "Test accuracy : 0.5043\n",
      "\n",
      "Epoch 38, Training loss: 1.664157\n",
      "Test accuracy : 0.5083\n",
      "\n",
      "Epoch 39, Training loss: 1.641662\n",
      "Test accuracy : 0.5104\n",
      "\n",
      "Epoch 40, Training loss: 1.619925\n",
      "Test accuracy : 0.5142\n",
      "\n",
      "Epoch 41, Training loss: 1.598917\n",
      "Test accuracy : 0.5177\n",
      "\n",
      "Epoch 42, Training loss: 1.578608\n",
      "Test accuracy : 0.5201\n",
      "\n",
      "Epoch 43, Training loss: 1.558966\n",
      "Test accuracy : 0.5246\n",
      "\n",
      "Epoch 44, Training loss: 1.539960\n",
      "Test accuracy : 0.5263\n",
      "\n",
      "Epoch 45, Training loss: 1.521559\n",
      "Test accuracy : 0.5302\n",
      "\n",
      "Epoch 46, Training loss: 1.503734\n",
      "Test accuracy : 0.5363\n",
      "\n",
      "Epoch 47, Training loss: 1.486455\n",
      "Test accuracy : 0.5393\n",
      "\n",
      "Epoch 48, Training loss: 1.469696\n",
      "Test accuracy : 0.5441\n",
      "\n",
      "Epoch 49, Training loss: 1.453431\n",
      "Test accuracy : 0.5492\n",
      "\n",
      "Epoch 50, Training loss: 1.437634\n",
      "Test accuracy : 0.5533\n",
      "\n",
      "Epoch 51, Training loss: 1.422284\n",
      "Test accuracy : 0.5556\n",
      "\n",
      "Epoch 52, Training loss: 1.407357\n",
      "Test accuracy : 0.5584\n",
      "\n",
      "Epoch 53, Training loss: 1.392835\n",
      "Test accuracy : 0.5619\n",
      "\n",
      "Epoch 54, Training loss: 1.378699\n",
      "Test accuracy : 0.5638\n",
      "\n",
      "Epoch 55, Training loss: 1.364932\n",
      "Test accuracy : 0.566\n",
      "\n",
      "Epoch 56, Training loss: 1.351518\n",
      "Test accuracy : 0.5693\n",
      "\n",
      "Epoch 57, Training loss: 1.338444\n",
      "Test accuracy : 0.5713\n",
      "\n",
      "Epoch 58, Training loss: 1.325696\n",
      "Test accuracy : 0.573\n",
      "\n",
      "Epoch 59, Training loss: 1.313262\n",
      "Test accuracy : 0.575\n",
      "\n",
      "Epoch 60, Training loss: 1.301133\n",
      "Test accuracy : 0.577\n",
      "\n",
      "Epoch 61, Training loss: 1.289297\n",
      "Test accuracy : 0.5795\n",
      "\n",
      "Epoch 62, Training loss: 1.277747\n",
      "Test accuracy : 0.5813\n",
      "\n",
      "Epoch 63, Training loss: 1.266473\n",
      "Test accuracy : 0.5832\n",
      "\n",
      "Epoch 64, Training loss: 1.255467\n",
      "Test accuracy : 0.5846\n",
      "\n",
      "Epoch 65, Training loss: 1.244722\n",
      "Test accuracy : 0.5865\n",
      "\n",
      "Epoch 66, Training loss: 1.234232\n",
      "Test accuracy : 0.5872\n",
      "\n",
      "Epoch 67, Training loss: 1.223988\n",
      "Test accuracy : 0.589\n",
      "\n",
      "Epoch 68, Training loss: 1.213986\n",
      "Test accuracy : 0.5904\n",
      "\n",
      "Epoch 69, Training loss: 1.204219\n",
      "Test accuracy : 0.5925\n",
      "\n",
      "Epoch 70, Training loss: 1.194681\n",
      "Test accuracy : 0.5948\n",
      "\n",
      "Epoch 71, Training loss: 1.185365\n",
      "Test accuracy : 0.5964\n",
      "\n",
      "Epoch 72, Training loss: 1.176267\n",
      "Test accuracy : 0.598\n",
      "\n",
      "Epoch 73, Training loss: 1.167380\n",
      "Test accuracy : 0.5994\n",
      "\n",
      "Epoch 74, Training loss: 1.158699\n",
      "Test accuracy : 0.6012\n",
      "\n",
      "Epoch 75, Training loss: 1.150219\n",
      "Test accuracy : 0.6028\n",
      "\n",
      "Epoch 76, Training loss: 1.141934\n",
      "Test accuracy : 0.605\n",
      "\n",
      "Epoch 77, Training loss: 1.133838\n",
      "Test accuracy : 0.6067\n",
      "\n",
      "Epoch 78, Training loss: 1.125927\n",
      "Test accuracy : 0.6088\n",
      "\n",
      "Epoch 79, Training loss: 1.118194\n",
      "Test accuracy : 0.6099\n",
      "\n",
      "Epoch 80, Training loss: 1.110636\n",
      "Test accuracy : 0.6121\n",
      "\n",
      "Epoch 81, Training loss: 1.103247\n",
      "Test accuracy : 0.6126\n",
      "\n",
      "Epoch 82, Training loss: 1.096022\n",
      "Test accuracy : 0.6143\n",
      "\n",
      "Epoch 83, Training loss: 1.088956\n",
      "Test accuracy : 0.6151\n",
      "\n",
      "Epoch 84, Training loss: 1.082044\n",
      "Test accuracy : 0.617\n",
      "\n",
      "Epoch 85, Training loss: 1.075282\n",
      "Test accuracy : 0.6183\n",
      "\n",
      "Epoch 86, Training loss: 1.068665\n",
      "Test accuracy : 0.6194\n",
      "\n",
      "Epoch 87, Training loss: 1.062189\n",
      "Test accuracy : 0.6205\n",
      "\n",
      "Epoch 88, Training loss: 1.055849\n",
      "Test accuracy : 0.6233\n",
      "\n",
      "Epoch 89, Training loss: 1.049642\n",
      "Test accuracy : 0.6254\n",
      "\n",
      "Epoch 90, Training loss: 1.043563\n",
      "Test accuracy : 0.6268\n",
      "\n",
      "Epoch 91, Training loss: 1.037608\n",
      "Test accuracy : 0.6286\n",
      "\n",
      "Epoch 92, Training loss: 1.031773\n",
      "Test accuracy : 0.6293\n",
      "\n",
      "Epoch 93, Training loss: 1.026056\n",
      "Test accuracy : 0.6319\n",
      "\n",
      "Epoch 94, Training loss: 1.020452\n",
      "Test accuracy : 0.6336\n",
      "\n",
      "Epoch 95, Training loss: 1.014957\n",
      "Test accuracy : 0.6357\n",
      "\n",
      "Epoch 96, Training loss: 1.009569\n",
      "Test accuracy : 0.6369\n",
      "\n",
      "Epoch 97, Training loss: 1.004285\n",
      "Test accuracy : 0.6379\n",
      "\n",
      "Epoch 98, Training loss: 0.999101\n",
      "Test accuracy : 0.639\n",
      "\n",
      "Epoch 99, Training loss: 0.994015\n",
      "Test accuracy : 0.6401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(X_train, Y_train, X_test, Y_test, epochs=10, learning_rate=0.01, activation='relu'):\n",
    "    np.random.seed(1) \n",
    "    nn = NeuralNetwork([X_train.shape[0], 64, 10], activation)\n",
    "    \n",
    "    # traing_loss, test_accuracy = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        AL, caches = nn.forward_propagation(X_train)\n",
    "        loss = nn.compute_loss(Y_train, AL)\n",
    "        grads = nn.backpropagation(X_train, Y_train, caches)\n",
    "        nn.update_parameters(grads, learning_rate)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch %i, Training loss: %f\" % (epoch, loss))\n",
    "    \n",
    "            # Evaluate model on whole test data after each epoch\n",
    "            predictions, _ = nn.forward_propagation(X_test)\n",
    "            accuracy = np.mean(np.argmax(predictions, axis=0) == np.argmax(Y_test, axis=0))\n",
    "            print(f\"Test accuracy : {accuracy}\\n\")\n",
    "            \n",
    "            # traing_loss.append(loss)\n",
    "            # test_accuracy.append(accuracy)\n",
    "    \n",
    "    # plot_training_loss_and_test_acc(epochs, traing_loss, test_accuracy)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = preprocess_data(train_images, train_labels, test_images, test_labels)\n",
    "train(X_train, Y_train, X_test, Y_test, epochs=100, activation='tanh', learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ef5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
